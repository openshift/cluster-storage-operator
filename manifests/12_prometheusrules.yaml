apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus
  namespace: openshift-cluster-storage-operator
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    capability.openshift.io/name: Storage
  labels:
    role: alert-rules
spec:
  groups:
    - name: default-storage-classes.rules
      rules:
      - alert: MultipleDefaultStorageClasses
        expr:  max_over_time(default_storage_class_count[5m]) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "More than one default StorageClass detected."
          description: |
            Cluster storage operator monitors all storage classes configured in the cluster
            and checks there is not more than one default StorageClass configured.
          message: "StorageClass count check is failing (there should not be more than one default StorageClass)"

    - name: storage-operations.rules
      rules:
      - alert: PodStartupStorageOperationsFailing
        # There was at least one failing operation in past 5 minutes *and* there was no successful operation.
        # To decide if there was no successful operation:
        # - either `increase(status = "success")` must be zero, if the metric has data.
        # - *or* if the metric has no data (= no operation for this volume plugin has even succeeded on this node),
        #   `or increase(state != "success") * 0` will report zero successes. We know a failure metric exists,
        #   so multiply it by zero to fill the gaps with zeroes.
        # Focus on attach and mount operations - they have the same diagnostic steps and are the most common.
        expr: |
          increase(storage_operation_duration_seconds_count{status != "success", operation_name =~"volume_attach|volume_mount"}[5m]) > 0
            and ignoring(status) (sum without(status)
              (increase(storage_operation_duration_seconds_count{status = "success", operation_name =~"volume_attach|volume_mount"}[5m]))
                or increase(storage_operation_duration_seconds_count{status != "success", operation_name =~"volume_attach|volume_mount"}[5m]) * 0
              ) == 0
        for: 5m
        labels:
          severity: info
        annotations:
          summary: "Pods can't start because {{ $labels.operation_name }} of volume plugin {{ $labels.volume_plugin }} is permanently failing on node {{ $labels.node }}."
          description: |
            Failing storage operation "{{ $labels.operation_name }}" of volume plugin {{ $labels.volume_plugin }} was preventing Pods on node {{ $labels.node }}
            from starting for past 5 minutes.
            Please investigate Pods that are "ContainerCreating" on the node: "oc get pod --field-selector=spec.nodeName={{ $labels.node }} --all-namespaces | grep ContainerCreating".
            Events of the Pods should contain exact error message: "oc describe pod -n <pod namespace> <pod name>".

    - name: storage-selinux.rules
      rules:
      # Two containers in a single pod have different contexts.
      - expr: sum(volume_manager_selinux_pod_context_mismatch_warnings_total) + sum(volume_manager_selinux_pod_context_mismatch_errors_total)
        record: cluster:volume_manager_selinux_pod_context_mismatch_total
      # Two pods use the same RWO / RWX volume, each with a different context.
      - expr: sum by(volume_plugin) (volume_manager_selinux_volume_context_mismatch_warnings_total{volume_plugin !~".*-e2e-.*"})
        record: cluster:volume_manager_selinux_volume_context_mismatch_warnings_total
      # Two pods use the same RWOP volume, each with a different context.
      - expr: sum by(volume_plugin) (volume_manager_selinux_volume_context_mismatch_errors_total{volume_plugin !~".*-e2e-.*"})
        record: cluster:volume_manager_selinux_volume_context_mismatch_errors_total
      # Pod with set SELinux context successfuly uses a volume (i.e. "mount -o context" would work).
      - expr: sum by(volume_plugin) (volume_manager_selinux_volumes_admitted_total{volume_plugin !~".*-e2e-.*"})
        record: cluster:volume_manager_selinux_volumes_admitted_total
