apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: vsphere-problem-detector
  namespace: openshift-cluster-storage-operator
  labels:
    role: alert-rules
spec:
  groups:
    # Alerting rules
    - name: vsphere-problem-detector.rules
      rules:
      - alert: VSphereOpenshiftNodeHealthFail
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr:  min_over_time(vsphere_node_check_errors[5m]) == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: |
            Periodic vSphere health check {{ $labels.check }} is failing on node {{ $labels.node }}.
            To get details about the failure, please see events in namespace openshift-cluster-storage-operator:
            'oc -n openshift-cluster-storage-operator get event --sort-by=.metadata.creationTimestamp | grep VSphere{{ $labels.check }}'
          message: "vSphere health check {{ $labels.check }} is failing on {{ $labels.node }}."
          description: |
            The vsphere-problem-detector performs health checks on individual OpenShift nodes on 
            vSphere to confirm configuration and performance requirements are met.  Health checks 
            verify machine scaling, storage provisioning, and node performance meet requirements.

      - alert: VSphereOpenshiftClusterHealthFail
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr: min_over_time(vsphere_cluster_check_errors[5m]) == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: |
            Periodic vSphere cluster health check {{ $labels.check }} is failing.
            To get details about the failure, please see events in namespace openshift-cluster-storage-operator:
            'oc -n openshift-cluster-storage-operator get event --sort-by=.metadata.creationTimestamp | grep VSphere{{ $labels.check }}'
          message: "vSphere cluster health checks are failing with {{ $labels.check }}"
          description: |
            The vsphere-problem-detector monitors the health, permissions and configuration of OpenShift on
            vSphere.  If problems are found which may prevent machine scaling, storage provisioning,
            and safe upgrades, the vsphere-problem-detector will raise alerts.

      - alert: VSphereOpenshiftConnectionFailure
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr: min_over_time(vsphere_sync_errors{reason =~ "UsernameWithoutDomain|UsernameWithNewLine|PasswordWithNewLine|InvalidCredentials"}[5m]) == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "vsphere-problem-detector is unable to connect to vSphere vCenter."
          description: |
            vsphere-problem-detector cannot access vCenter. As consequence, other OCP components,
            such as storage or machine API, may not be able to access vCenter too and provide
            their services. Detailed error message can be found in Available condition of
            ClusterOperator "storage", either in console
            (Administration -> Cluster settings -> Cluster operators tab -> storage) or on
            command line: oc get clusteroperator storage -o jsonpath='{.status.conditions[?(@.type=="Available")].message}'

      - alert: CSIWithOldVSphereHWVersion
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr: |
          min_over_time(vsphere_node_hw_version_total{hw_version=~"vmx-(11|12|13|14)"}[5m]) > 0
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Detected vSphere VM with HW version lower than 15, which is being deprecated by Openshift."
          description: |
            The cluster is using VMs with hardware version lower than 15, which is being deprecated by Openshift. Hardware version 15 or greater
            is required by vSphere CSI driver. Please update your VMs to at least HW version 15.

      - alert: VSphereOlderVersionPresent
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr: |
          min_over_time(vsphere_esxi_version_total{api_version=~"^7\\.0\\.[0-1].*|^6.*"}[5m]) > 0
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Detected vSphere host with ESXi version less than 7.0.2 in Openshift cluster."
          description: |
            The cluster is using ESXi hosts which are on version less than 7.0.2, which is being deprecated by Openshift. A future version of
            Openshift will remove support for ESXi version less than 7.0.2 and it is recommended to update your hosts to the latest ESXi version.

      - alert: VSphereOlderVCenterPresent
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr: |
          min_over_time(vsphere_vcenter_info{api_version=~"^7\\.0\\.[0-1].*|^6.*"}[5m]) > 0
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Detected vSphere vCenter version less than 7.0.2 in Openshift cluster."
          description: |
            The cluster is using vCenter version less than 7.0.2, which is being deprecated by Openshift. A future version of
            Openshift will remove support for vCenter versions lest than 7.0.2 and it is recommended to update your vCenter to the latest version.

      - alert: VSphereOpenshiftVmsCBTMismatch
        # Using min_over_time to make sure the metric is `1` for whole 5 minutes.
        # A missed scraping (e.g. due to a pod restart) will result in prometheus re-evaluating the the alerting rule.
        expr:  min_over_time(vsphere_vm_cbt_checks{cbt=~"enabled"}[5m]) > 0 and on() min_over_time(vsphere_vm_cbt_checks{cbt=~"disabled"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: |
            Periodic vSphere health check is failing due to some nodes not having ctkEnabled matching the other nodes.
            To get details about the failure, please see the logs in the vsphere-problem-detector-operator pod in namespace openshift-cluster-storage-operator:
            ' oc logs -l name=vsphere-problem-detector-operator -n openshift-cluster-storage-operator --tail=-1 | grep "node_cbt"'
          message: "Cluster node VMs are not configured the same for CBT feature."
          description: |
            Cluster node VMs are not configured the same for CBT feature.
            
